<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../../../img/favicon.ico">
  <title>Testing Python Data Science pipelines - DSSG Hitchhickers guide</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Testing Python Data Science pipelines";
    var mkdocs_page_input_path = "curriculum/4_programming_best_practices/test-test-test/ds_testing.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../../../.." class="icon icon-home"> DSSG Hitchhickers guide</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../../../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">DSSG Manual</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../../dssg-manual/conduct-culture-and-communications/">Code of conduct</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../../dssg-manual/summer-overview/">Summer overview</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../">Curriculum</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../../..">DSSG Hitchhickers guide</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../..">Docs</a> &raquo;</li>
    
      
    
    <li>Testing Python Data Science pipelines</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/dssg/hitchhikers-guide/blob/master/docs/templates/curriculum/4_programming_best_practices/test-test-test/ds_testing.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="testing-python-data-science-pipelines">Testing Python Data Science pipelines<a class="headerlink" href="#testing-python-data-science-pipelines" title="Permanent link">&para;</a></h2>
<p>Testing Data Pipelines is <strong>hard</strong>. By definition you do not expect them to produce the same output over and over again. There isn't a standard way of doing it (or at least I don't know it), but here you'll find some tips to test your pipeline steps.</p>
<h3 id="tips-to-test-your-pipeline">Tips to test your pipeline<a class="headerlink" href="#tips-to-test-your-pipeline" title="Permanent link">&para;</a></h3>
<h4 id="separate-the-source-code-and-your-pipeline-steps-in-different-modules">Separate the source code and your pipeline steps in different modules<a class="headerlink" href="#separate-the-source-code-and-your-pipeline-steps-in-different-modules" title="Permanent link">&para;</a></h4>
<p>As it was mentioned in <a href="../python_testing/">Part1</a>. It's important to separate the source code from your pipeline. Think of source code as the building blocks for processing data, uploading to the database, training models and so on. For example, the training step in your pipeline may look like this:</p>
<pre><code class="python"># training step in your pipeline

# load features
train, test = load_features(['feature1', 'feature2', 'feature3'])

# get a list of models to train
to_train = load_models('RandomForest', 'LogisticRegression')

# train every model
for model in to_train:
    # fit the model
    model.fit(train.y, train.y)
    # evaluate some metrics on the test set,
    # save results in a database,
    # create HTML/PDF reports
    evaluate_model(model, test)
</code></pre>

<p>In the training step above <code>load_features</code>, <code>load_models</code> and <code>evaluate_model</code> depend on your project, maybe you are loading data from PostgresSQL, maybe from HDF5. The models you train also depend on your project and the evaluation depends on which metrics are best suited for your project's goal.</p>
<p>Those functions are building blocks and the <em>source code</em> for those should be outside your training script. Probably the <code>load_features</code> function does a lot of data transformations, try to divide it in various <em>small</em> functions and run <em>unit tests</em> on them.</p>
<h4 id="make-the-data-assumptions-in-your-code-explicit">Make the data assumptions in your code explicit<a class="headerlink" href="#make-the-data-assumptions-in-your-code-explicit" title="Permanent link">&para;</a></h4>
<p>When working on your pipeline the math/logic may work fine, but what if you are feeding the wrong data? Imagine what would happen if you train a classifier and you forgot to delete the outcome variable in your feature set, that sounds like a dumb mistake, but as your pipeline gets more and more complex, <em>it can happen</em>.</p>
<p>To prevent those things from happening you should <em>test your pipeline at runtime</em>, meaning that you should check for red flags while training models. Let's see an example.</p>
<pre><code class="python">def load_features(list_of_features):
    &quot;&quot;&quot;
        This function loads features from the database
    &quot;&quot;&quot;
    uri = load_uri()
    con = db_connection(uri)
    tables = load_from_db(list_of_features, con)
    if 'outcome' in tables:
        raise Exception('You cannot load the outcome variable as a feature')
    if any(tables, has_nas):
        raise Exception('You cannot load features with NAs')
    return tables
</code></pre>

<p>In the snippet above we are making two assumptions explicit, the first one is that we shouldn't load a column named 'outcome' in our feature set, the second one means that we cannot load columns with NAs, because this may break the following steps in our pipeline.</p>
<h4 id="test-your-code-with-a-fake-database">Test your code with a fake database<a class="headerlink" href="#test-your-code-with-a-fake-database" title="Permanent link">&para;</a></h4>
<p>Imagine that you implemented a function to perform some complex feature engineering in your database, then you modify some parts to make it faster and you have a test to check that the results hold the same. If your database has millions of rows and a couple hundred features, how long is the test going to take?</p>
<p>When testing code that interacts with a lot of data is often a good idea to sample it and put it in a test database, that way you can test faster, but don't force yourself to make your tests run fast. Always remember <em>a fast test is better than slow test, but a slow test is better than not testing at all</em>.</p>
<p>How do you change which database your code uses? There are a couple of ways, you can for example define an <a href="https://en.wikipedia.org/wiki/Environment_variable">environment variable</a> to change the behavior of your <code>open_db_connection</code> function.</p>
<p><u>Note: use environmental variables judiciously and don't store any sensitive data.</u></p>
<pre><code class="python"># db.py
import os

def db_connection():
    if os.environ['testing_mode']:
        return connect_to_testing_db()
    else:
        return connect_to_production_db()
</code></pre>

<p>Now, you need to add some custom logic to the script that runs your tests, let's see how to do it:</p>
<pre><code class="bash"># run_tests.sh
export testing_mode=YES
py.test # run your tests
export testing_mode=NO
</code></pre>

<h4 id="dealing-with-randomness">Dealing with randomness<a class="headerlink" href="#dealing-with-randomness" title="Permanent link">&para;</a></h4>
<p>The hardest part of testing pipelines is dealing with randomness, how do you test a random generator function? or a probabilistic function? One of the simplest ways to do it is to take out the randomness by setting the <a href="https://en.wikipedia.org/wiki/Random_seed">random seed</a> in your tests. Let's see an example.</p>
<pre><code class="python"># random.py
def generate_random_number_between(a, b):
    # generate random number using seed value,
    # a and b
    return number
</code></pre>

<pre><code class="python"># test_random.py

# set the seed value so you always get random numbers in the same order
# during the tests
set_random_seed(0)

def test_generate_random_number_below_10():
    assert generate_random_number_between(0, 10) == 5

def test_generate_random_number_between_8_12():
    assert generate_random_number_between(8, 12) == 10
</code></pre>

<p>The example above is a bit naive, but it hopefully gives you an idea on how to take out the randomness by setting the seed, let's see a more robust example.</p>
<p>Another approach is to test your function enough number and check the result against an interval and not an specific value. Let's see how to test a function that draws one sample from the normal distribution:</p>
<pre><code class="python"># normal_sample.py
def normal_dist_sample(mean=0, std=1):
    # do stuff
    return sample
</code></pre>

<pre><code class="python"># test_normal_sample.py
import numpy.testing as npt

def test_normal_dist_sample_mean():
    # draw 1000 samples
    samples = [normal_dist_sample() for i in range(10000)]
    # calculate the mean
    mean = samples.mean()
    # check that the mean is almost equal to zero
    assert npt.assert_almost_equal(mean, 0)
</code></pre>

<p>As you can see, testing probabilistic code is not trivial, so do it only when your project highly depends on such functions. But make sure you write unit tests and to check the assumptions in your data!</p>
<h3 id="tools-for-testing">Tools for testing<a class="headerlink" href="#tools-for-testing" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://github.com/HypothesisWorks/hypothesis-python">hypothesis</a> - Python library to look for edge cases without explicitly coding them</li>
<li><a href="https://github.com/TomAugspurger/engarde">engarde</a> - Use Python decorators to test a function outcome (this project had good potential but it's dead now)</li>
<li><a href="https://github.com/machinalis/featureforge">feature forge</a> - Testing features for ML models (also seems dead)</li>
</ul>
<h3 id="external-resources">External resources<a class="headerlink" href="#external-resources" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://www.youtube.com/watch?v=GEqM9uJi64Q">Testing for Data Scientists by Trey Causey</a></li>
</ul>
<h4 id="where-to-go-from-here">Where to go from here<a class="headerlink" href="#where-to-go-from-here" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Part 3:</strong> <a href="../ci/">Continuous Integration</a></li>
</ul>
              
            </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/dssg/hitchhikers-guide/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
      
    </span>
</div>
    <script>var base_url = '../../../..';</script>
    <script src="../../../../js/theme.js" defer></script>
      <script src="../../../../js/mermaid.min.js" defer></script>
      <script src="../../../../search/main.js" defer></script>

</body>
</html>
